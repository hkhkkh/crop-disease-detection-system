#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›çš„YOLOä½œç‰©ç—…å®³æ£€æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬
åŸºäºYOLOv8è¿›è¡Œä½œç‰©ç—…å®³æ£€æµ‹æ¨¡å‹çš„è®­ç»ƒå’Œä¼˜åŒ–
"""

import os
import sys
import torch
import yaml
import shutil
import argparse
from pathlib import Path
from datetime import datetime
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import cv2
from PIL import Image
import random

# æ·»åŠ ultralyticsæ”¯æŒ
try:
    from ultralytics import YOLO
    from ultralytics.utils.plotting import Annotator, colors
except ImportError:
    print("âŒ è¯·å®‰è£…ultralytics: pip install ultralytics")
    sys.exit(1)

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

class CropDiseaseYOLOTrainer:
    def __init__(self, config):
        """åˆå§‹åŒ–YOLOè®­ç»ƒå™¨"""
        self.config = config
        self.dataset_path = Path(config['dataset_path'])
        self.output_dir = Path(config['output_dir'])
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®éšæœºç§å­
        self.set_random_seed(config.get('random_seed', 42))
        
        # ç±»åˆ«ä¿¡æ¯
        self.class_names = []
        self.class_mapping = {}
        
        print(f"ğŸ¯ YOLOè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“‚ æ•°æ®é›†è·¯å¾„: {self.dataset_path}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        
    def set_random_seed(self, seed):
        """è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯å¤ç°"""
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
        print(f"ğŸ² éšæœºç§å­è®¾ç½®ä¸º: {seed}")
        
    def prepare_dataset(self):
        """å‡†å¤‡YOLOæ ¼å¼çš„æ•°æ®é›†"""
        print("\nğŸ”„ å‡†å¤‡YOLOæ•°æ®é›†...")
        
        # åˆ›å»ºYOLOæ•°æ®é›†ç›®å½•ç»“æ„
        yolo_dir = self.output_dir / "yolo_dataset"
        train_img_dir = yolo_dir / "train" / "images"
        train_label_dir = yolo_dir / "train" / "labels"
        val_img_dir = yolo_dir / "val" / "images"
        val_label_dir = yolo_dir / "val" / "labels"
        test_img_dir = yolo_dir / "test" / "images"
        test_label_dir = yolo_dir / "test" / "labels"
        
        # åˆ›å»ºç›®å½•
        for dir_path in [train_img_dir, train_label_dir, val_img_dir, val_label_dir, test_img_dir, test_label_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
            
        # è·å–æ‰€æœ‰ç±»åˆ«
        class_dirs = [d for d in self.dataset_path.iterdir() if d.is_dir()]
        self.class_names = sorted([d.name for d in class_dirs])
        self.class_mapping = {name: idx for idx, name in enumerate(self.class_names)}
        
        print(f"ğŸ“‹ å‘ç° {len(self.class_names)} ä¸ªç±»åˆ«")
        
        # å¤„ç†æ¯ä¸ªç±»åˆ«çš„å›¾åƒ
        all_image_paths = []
        all_labels = []
        
        for class_dir in class_dirs:
            class_name = class_dir.name
            class_id = self.class_mapping[class_name]
            
            # è·å–è¯¥ç±»åˆ«çš„æ‰€æœ‰å›¾åƒ
            image_files = list(class_dir.glob("*.jpg")) + list(class_dir.glob("*.jpeg")) + list(class_dir.glob("*.png"))
            
            for img_file in image_files:
                all_image_paths.append(str(img_file))
                all_labels.append(class_id)
                
        print(f"ğŸ“Š æ€»å…±å¤„ç† {len(all_image_paths)} å¼ å›¾åƒ")
        
        # åˆ’åˆ†è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›† (7:2:1)
        train_ratio = self.config.get('train_ratio', 0.7)
        val_ratio = self.config.get('val_ratio', 0.2)
        test_ratio = 1 - train_ratio - val_ratio
        
        # é¦–å…ˆåˆ†ç¦»è®­ç»ƒé›†å’Œä¸´æ—¶é›†
        train_paths, temp_paths, train_labels, temp_labels = train_test_split(
            all_image_paths, all_labels, 
            train_size=train_ratio, 
            stratify=all_labels, 
            random_state=42
        )
        
        # å†ä»ä¸´æ—¶é›†åˆ†ç¦»éªŒè¯é›†å’Œæµ‹è¯•é›†
        val_paths, test_paths, val_labels, test_labels = train_test_split(
            temp_paths, temp_labels,
            train_size=val_ratio/(val_ratio + test_ratio),
            stratify=temp_labels,
            random_state=42
        )
        
        print(f"ğŸ“ˆ æ•°æ®é›†åˆ’åˆ†:")
        print(f"  è®­ç»ƒé›†: {len(train_paths)} å¼  ({len(train_paths)/len(all_image_paths)*100:.1f}%)")
        print(f"  éªŒè¯é›†: {len(val_paths)} å¼  ({len(val_paths)/len(all_image_paths)*100:.1f}%)")
        print(f"  æµ‹è¯•é›†: {len(test_paths)} å¼  ({len(test_paths)/len(all_image_paths)*100:.1f}%)")
        
        # å¤åˆ¶å›¾åƒå¹¶åˆ›å»ºæ ‡ç­¾
        def copy_images_and_create_labels(paths, labels, img_dir, label_dir, split_name):
            print(f"ğŸ”„ å¤„ç†{split_name}é›†...")
            for img_path, label in zip(paths, labels):
                # å¤åˆ¶å›¾åƒ
                img_file = Path(img_path)
                dst_img_path = img_dir / img_file.name
                shutil.copy2(img_path, dst_img_path)
                
                # åˆ›å»ºYOLOæ ¼å¼æ ‡ç­¾ (å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œæˆ‘ä»¬åˆ›å»ºæ•´å›¾æ ‡æ³¨)
                label_file = label_dir / f"{img_file.stem}.txt"
                
                # è¯»å–å›¾åƒå°ºå¯¸
                img = cv2.imread(img_path)
                h, w = img.shape[:2]
                
                # YOLOæ ¼å¼: class_id center_x center_y width height (å½’ä¸€åŒ–)
                # å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œæˆ‘ä»¬å°†æ•´ä¸ªå›¾åƒä½œä¸ºä¸€ä¸ªè¾¹ç•Œæ¡†
                with open(label_file, 'w') as f:
                    f.write(f"{label} 0.5 0.5 1.0 1.0\n")
                    
        copy_images_and_create_labels(train_paths, train_labels, train_img_dir, train_label_dir, "è®­ç»ƒ")
        copy_images_and_create_labels(val_paths, val_labels, val_img_dir, val_label_dir, "éªŒè¯")
        copy_images_and_create_labels(test_paths, test_labels, test_img_dir, test_label_dir, "æµ‹è¯•")
        
        # åˆ›å»ºç±»åˆ«æ–‡ä»¶
        with open(yolo_dir / "classes.txt", 'w', encoding='utf-8') as f:
            for class_name in self.class_names:
                f.write(f"{class_name}\n")
                
        # åˆ›å»ºYOLOæ•°æ®é…ç½®æ–‡ä»¶
        yaml_content = {
            'path': str(yolo_dir.absolute()),
            'train': 'train/images',
            'val': 'val/images',
            'test': 'test/images',
            'nc': len(self.class_names),
            'names': self.class_names
        }
        
        with open(yolo_dir / "dataset.yaml", 'w', encoding='utf-8') as f:
            yaml.dump(yaml_content, f, default_flow_style=False, allow_unicode=True)
            
        self.yolo_dataset_path = yolo_dir / "dataset.yaml"
        print(f"âœ… YOLOæ•°æ®é›†å‡†å¤‡å®Œæˆ: {yolo_dir}")
        
        return yolo_dir
        
    def create_improved_model_config(self):
        """åˆ›å»ºæ”¹è¿›çš„YOLOæ¨¡å‹é…ç½®"""
        print("\nâš™ï¸ åˆ›å»ºæ”¹è¿›çš„æ¨¡å‹é…ç½®...")
        
        # åŸºäºä½œç‰©ç—…å®³æ£€æµ‹ä»»åŠ¡çš„ç‰¹ç‚¹å®šåˆ¶é…ç½®
        improved_config = {
            # æ¨¡å‹ç»“æ„ä¼˜åŒ–
            'model': {
                'type': 'YOLOv8',
                'size': self.config.get('model_size', 'n'),  # n, s, m, l, x
                'pretrained': True,
                'freeze_backbone': False,  # æ˜¯å¦å†»ç»“backbone
            },
            
            # è®­ç»ƒè¶…å‚æ•°ä¼˜åŒ–
            'training': {
                'epochs': self.config.get('epochs', 100),
                'batch_size': self.config.get('batch_size', 16),
                'imgsz': self.config.get('img_size', 640),
                'optimizer': 'AdamW',  # ä½¿ç”¨AdamWä¼˜åŒ–å™¨
                'lr0': 0.001,  # åˆå§‹å­¦ä¹ ç‡
                'lrf': 0.01,   # æœ€ç»ˆå­¦ä¹ ç‡å› å­
                'momentum': 0.937,
                'weight_decay': 0.0005,
                'warmup_epochs': 3,
                'warmup_momentum': 0.8,
                'warmup_bias_lr': 0.1,
            },
            
            # æ•°æ®å¢å¼ºç­–ç•¥
            'augmentation': {
                'hsv_h': 0.015,      # è‰²è°ƒå¢å¼º
                'hsv_s': 0.7,        # é¥±å’Œåº¦å¢å¼º
                'hsv_v': 0.4,        # äº®åº¦å¢å¼º
                'degrees': 10.0,     # æ—‹è½¬è§’åº¦
                'translate': 0.1,    # å¹³ç§»
                'scale': 0.5,        # ç¼©æ”¾
                'shear': 0.0,        # å‰ªåˆ‡
                'perspective': 0.0,  # é€è§†å˜æ¢
                'flipud': 0.5,       # å‚ç›´ç¿»è½¬
                'fliplr': 0.5,       # æ°´å¹³ç¿»è½¬
                'mosaic': 1.0,       # é©¬èµ›å…‹å¢å¼º
                'mixup': 0.1,        # mixupå¢å¼º
                'copy_paste': 0.1,   # å¤åˆ¶ç²˜è´´å¢å¼º
            },
            
            # æŸå¤±å‡½æ•°ä¼˜åŒ–
            'loss': {
                'cls': 0.5,          # åˆ†ç±»æŸå¤±æƒé‡
                'box': 7.5,          # è¾¹ç•Œæ¡†æŸå¤±æƒé‡
                'dfl': 1.5,          # DFLæŸå¤±æƒé‡
                'focal_loss_gamma': 1.5,  # Focal loss gamma
                'label_smoothing': 0.1,   # æ ‡ç­¾å¹³æ»‘
            }
        }
        
        # ä¿å­˜é…ç½®
        config_file = self.output_dir / "improved_model_config.yaml"
        with open(config_file, 'w', encoding='utf-8') as f:
            yaml.dump(improved_config, f, default_flow_style=False, allow_unicode=True)
            
        print(f"ğŸ’¾ æ”¹è¿›é…ç½®å·²ä¿å­˜: {config_file}")
        return improved_config
        
    def train_model(self):
        """è®­ç»ƒYOLOæ¨¡å‹"""
        print("\nğŸš€ å¼€å§‹è®­ç»ƒYOLOæ¨¡å‹...")
        
        # åˆ›å»ºæ”¹è¿›é…ç½®
        improved_config = self.create_improved_model_config()
        
        # é€‰æ‹©æ¨¡å‹å¤§å°
        model_size = improved_config['model']['size']
        model_name = f"yolov8{model_size}.pt"
        
        print(f"ğŸ“¦ ä½¿ç”¨æ¨¡å‹: {model_name}")
        
        # åˆå§‹åŒ–YOLOæ¨¡å‹
        model = YOLO(model_name)
        
        # è®­ç»ƒå‚æ•°
        train_params = {
            'data': str(self.yolo_dataset_path),
            'epochs': improved_config['training']['epochs'],
            'batch': improved_config['training']['batch_size'],
            'imgsz': improved_config['training']['imgsz'],
            'optimizer': improved_config['training']['optimizer'],
            'lr0': improved_config['training']['lr0'],
            'lrf': improved_config['training']['lrf'],
            'momentum': improved_config['training']['momentum'],
            'weight_decay': improved_config['training']['weight_decay'],
            'warmup_epochs': improved_config['training']['warmup_epochs'],
            'warmup_momentum': improved_config['training']['warmup_momentum'],
            'warmup_bias_lr': improved_config['training']['warmup_bias_lr'],
            'project': str(self.output_dir),
            'name': f'crop_disease_yolo_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
            'save': True,
            'save_period': 10,  # æ¯10ä¸ªepochä¿å­˜ä¸€æ¬¡
            'cache': False,     # ä¸ç¼“å­˜å›¾åƒåˆ°å†…å­˜
            'device': 'cuda' if torch.cuda.is_available() else 'cpu',
            'workers': 8,
            'verbose': True,
            'seed': 42,
            'deterministic': True,
        }
        
        # åº”ç”¨æ•°æ®å¢å¼ºå‚æ•°
        for key, value in improved_config['augmentation'].items():
            train_params[key] = value
            
        # åº”ç”¨æŸå¤±å‡½æ•°å‚æ•°
        for key, value in improved_config['loss'].items():
            train_params[key] = value
            
        print("ğŸ”§ è®­ç»ƒå‚æ•°:")
        for key, value in train_params.items():
            print(f"  {key}: {value}")
            
        # å¼€å§‹è®­ç»ƒ
        print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
        results = model.train(**train_params)
        
        # ä¿å­˜è®­ç»ƒç»“æœ
        self.trained_model = model
        self.training_results = results
        
        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        return results
        
    def evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("\nğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        if not hasattr(self, 'trained_model'):
            print("âŒ æ²¡æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒ")
            return None
            
        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
        val_results = self.trained_model.val()
        
        # åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæ¨ç†å¹¶è¯„ä¼°
        test_img_dir = self.output_dir / "yolo_dataset" / "test" / "images"
        test_images = list(test_img_dir.glob("*.jpg")) + list(test_img_dir.glob("*.png"))
        
        if test_images:
            print(f"ğŸ§ª åœ¨ {len(test_images)} å¼ æµ‹è¯•å›¾åƒä¸Šè¯„ä¼°...")
            
            # æ‰¹é‡é¢„æµ‹
            results = self.trained_model.predict(source=str(test_img_dir), save=False, verbose=False)
            
            # æ”¶é›†é¢„æµ‹ç»“æœ
            predictions = []
            true_labels = []
            
            for i, result in enumerate(results):
                # è·å–å›¾åƒæ–‡ä»¶å
                img_path = test_images[i]
                
                # è¯»å–çœŸå®æ ‡ç­¾
                label_file = test_img_dir.parent / "labels" / f"{img_path.stem}.txt"
                if label_file.exists():
                    with open(label_file, 'r') as f:
                        true_class = int(f.readline().strip().split()[0])
                        true_labels.append(true_class)
                        
                    # è·å–é¢„æµ‹ç»“æœ
                    if len(result.boxes) > 0:
                        pred_class = int(result.boxes.cls[0].cpu())
                        predictions.append(pred_class)
                    else:
                        predictions.append(-1)  # æœªæ£€æµ‹åˆ°
                        
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            if predictions and true_labels:
                self.calculate_metrics(true_labels, predictions)
                
        return val_results
        
    def calculate_metrics(self, true_labels, predictions):
        """è®¡ç®—è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡"""
        print("\nğŸ“ˆ è®¡ç®—æ€§èƒ½æŒ‡æ ‡...")
        
        # è¿‡æ»¤æ‰æœªæ£€æµ‹åˆ°çš„æ ·æœ¬
        valid_indices = [i for i, pred in enumerate(predictions) if pred != -1]
        filtered_true = [true_labels[i] for i in valid_indices]
        filtered_pred = [predictions[i] for i in valid_indices]
        
        if not filtered_true:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹ç»“æœ")
            return
            
        # è®¡ç®—åˆ†ç±»æŠ¥å‘Š
        from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
        
        accuracy = accuracy_score(filtered_true, filtered_pred)
        precision, recall, f1, support = precision_recall_fscore_support(filtered_true, filtered_pred, average='weighted')
        
        print(f"ğŸ¯ æ•´ä½“æ€§èƒ½æŒ‡æ ‡:")
        print(f"  å‡†ç¡®ç‡ (Accuracy): {accuracy:.4f}")
        print(f"  ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
        print(f"  å¬å›ç‡ (Recall): {recall:.4f}")
        print(f"  F1åˆ†æ•°: {f1:.4f}")
        print(f"  æ£€æµ‹ç‡: {len(filtered_true)}/{len(true_labels)} ({len(filtered_true)/len(true_labels)*100:.1f}%)")
        
        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        class_names_filtered = [self.class_names[i] for i in range(len(self.class_names)) if i in set(filtered_true + filtered_pred)]
        report = classification_report(filtered_true, filtered_pred, target_names=class_names_filtered)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.output_dir / "performance_report.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# ä½œç‰©ç—…å®³æ£€æµ‹æ¨¡å‹æ€§èƒ½æŠ¥å‘Š\n\n")
            f.write(f"## æ•´ä½“æ€§èƒ½\n")
            f.write(f"- å‡†ç¡®ç‡: {accuracy:.4f}\n")
            f.write(f"- ç²¾ç¡®ç‡: {precision:.4f}\n")
            f.write(f"- å¬å›ç‡: {recall:.4f}\n")
            f.write(f"- F1åˆ†æ•°: {f1:.4f}\n")
            f.write(f"- æ£€æµ‹ç‡: {len(filtered_true)}/{len(true_labels)} ({len(filtered_true)/len(true_labels)*100:.1f}%)\n\n")
            f.write(f"## è¯¦ç»†åˆ†ç±»æŠ¥å‘Š\n")
            f.write(report)
            
        print(f"ğŸ“„ æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        self.plot_confusion_matrix(filtered_true, filtered_pred)
        
    def plot_confusion_matrix(self, true_labels, predictions):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        from sklearn.metrics import confusion_matrix
        
        cm = confusion_matrix(true_labels, predictions)
        
        plt.figure(figsize=(12, 10))
        
        # åªæ˜¾ç¤ºå‡ºç°åœ¨æµ‹è¯•é›†ä¸­çš„ç±»åˆ«
        unique_labels = sorted(set(true_labels + predictions))
        class_names_subset = [self.class_names[i] for i in unique_labels]
        
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=class_names_subset,
                   yticklabels=class_names_subset)
        plt.title('æ··æ·†çŸ©é˜µ')
        plt.xlabel('é¢„æµ‹ç±»åˆ«')
        plt.ylabel('çœŸå®ç±»åˆ«')
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        cm_file = self.output_dir / "confusion_matrix.png"
        plt.savefig(cm_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š æ··æ·†çŸ©é˜µå·²ä¿å­˜: {cm_file}")
        
    def save_final_model(self):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        print("\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
        
        if not hasattr(self, 'trained_model'):
            print("âŒ æ²¡æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹")
            return
            
        # ä¿å­˜æ¨¡å‹æƒé‡
        model_file = self.output_dir / "best_crop_disease_model.pt"
        self.trained_model.export(format='onnx')  # å¯¼å‡ºONNXæ ¼å¼
        
        # å¤åˆ¶æœ€ä½³æƒé‡
        runs_dir = self.output_dir / "runs" / "detect"
        if runs_dir.exists():
            latest_run = max(runs_dir.glob("crop_disease_yolo_*"), key=os.path.getctime, default=None)
            if latest_run and (latest_run / "weights" / "best.pt").exists():
                shutil.copy2(latest_run / "weights" / "best.pt", model_file)
                print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {model_file}")
                
        # ä¿å­˜æ¨¡å‹ä¿¡æ¯
        model_info = {
            'model_type': 'YOLOv8',
            'num_classes': len(self.class_names),
            'class_names': self.class_names,
            'image_size': self.config.get('img_size', 640),
            'training_date': datetime.now().isoformat(),
            'config': self.config
        }
        
        info_file = self.output_dir / "model_info.yaml"
        with open(info_file, 'w', encoding='utf-8') as f:
            yaml.dump(model_info, f, default_flow_style=False, allow_unicode=True)
            
        print(f"ğŸ“‹ æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜: {info_file}")


def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    parser = argparse.ArgumentParser(description='YOLOä½œç‰©ç—…å®³æ£€æµ‹æ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--dataset', type=str, 
                       default=r"F:\photos\Data for Identification of Plant Leaf Diseases Using a 9-layer Deep Convolutional Neural Network\Plant_leaf_diseases_dataset_with_augmentation\Plant_leave_diseases_dataset_with_augmentation",
                       help='æ•°æ®é›†è·¯å¾„')
    parser.add_argument('--output', type=str, default='model-training/outputs', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=16, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--img-size', type=int, default=640, help='å›¾åƒå°ºå¯¸')
    parser.add_argument('--model-size', type=str, default='n', choices=['n', 's', 'm', 'l', 'x'], help='æ¨¡å‹å¤§å°')
    
    args = parser.parse_args()
    
    # é…ç½®å‚æ•°
    config = {
        'dataset_path': args.dataset,
        'output_dir': args.output,
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'img_size': args.img_size,
        'model_size': args.model_size,
        'train_ratio': 0.7,
        'val_ratio': 0.2,
        'random_seed': 42
    }
    
    print("ğŸŒ± YOLOä½œç‰©ç—…å®³æ£€æµ‹æ¨¡å‹è®­ç»ƒ")
    print("=" * 50)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = CropDiseaseYOLOTrainer(config)
    
    try:
        # 1. å‡†å¤‡æ•°æ®é›†
        trainer.prepare_dataset()
        
        # 2. è®­ç»ƒæ¨¡å‹
        trainer.train_model()
        
        # 3. è¯„ä¼°æ¨¡å‹
        trainer.evaluate_model()
        
        # 4. ä¿å­˜æœ€ç»ˆæ¨¡å‹
        trainer.save_final_model()
        
        print("\nğŸ‰ è®­ç»ƒæµç¨‹å®Œæˆ!")
        print(f"ğŸ“ æŸ¥çœ‹ç»“æœ: {trainer.output_dir}")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
