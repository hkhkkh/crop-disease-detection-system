#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é›†åˆ†æä¸å¤„ç†è„šæœ¬
ç”¨äºåˆ†ææ¤ç‰©ç—…å®³æ•°æ®é›†çš„ç»“æ„å’Œç»Ÿè®¡ä¿¡æ¯
"""

import os
import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from collections import Counter
import cv2
import numpy as np
from PIL import Image
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

class DatasetAnalyzer:
    def __init__(self, dataset_path):
        """åˆå§‹åŒ–æ•°æ®é›†åˆ†æå™¨"""
        self.dataset_path = Path(dataset_path)
        self.class_info = {}
        self.total_images = 0
        
    def analyze_dataset_structure(self):
        """åˆ†ææ•°æ®é›†ç»“æ„"""
        print("ğŸ” å¼€å§‹åˆ†ææ•°æ®é›†ç»“æ„...")
        
        if not self.dataset_path.exists():
            print(f"âŒ æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {self.dataset_path}")
            return False
            
        # è·å–æ‰€æœ‰ç±»åˆ«ç›®å½•
        class_dirs = [d for d in self.dataset_path.iterdir() if d.is_dir()]
        
        print(f"ğŸ“Š å‘ç° {len(class_dirs)} ä¸ªç±»åˆ«:")
        
        for class_dir in sorted(class_dirs):
            # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„å›¾ç‰‡æ•°é‡
            image_files = list(class_dir.glob("*.jpg")) + list(class_dir.glob("*.jpeg")) + list(class_dir.glob("*.png"))
            image_count = len(image_files)
            
            # è§£æç±»åˆ«ä¿¡æ¯
            class_name = class_dir.name
            if "___" in class_name:
                crop_type, disease_name = class_name.split("___", 1)
            else:
                crop_type = "Unknown"
                disease_name = class_name
                
            self.class_info[class_name] = {
                'crop_type': crop_type,
                'disease_name': disease_name,
                'image_count': image_count,
                'path': str(class_dir)
            }
            
            self.total_images += image_count
            print(f"  ğŸ“ {class_name}: {image_count} å¼ å›¾ç‰‡")
            
        print(f"\nğŸ“ˆ æ€»è®¡: {self.total_images} å¼ å›¾ç‰‡")
        return True
        
    def generate_statistics(self):
        """ç”Ÿæˆæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        print("\nğŸ“Š ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯...")
        
        # æŒ‰ä½œç‰©ç±»å‹ç»Ÿè®¡
        crop_stats = {}
        disease_stats = {}
        
        for class_name, info in self.class_info.items():
            crop_type = info['crop_type']
            disease_name = info['disease_name']
            count = info['image_count']
            
            # ä½œç‰©ç±»å‹ç»Ÿè®¡
            if crop_type not in crop_stats:
                crop_stats[crop_type] = {'classes': 0, 'images': 0}
            crop_stats[crop_type]['classes'] += 1
            crop_stats[crop_type]['images'] += count
            
            # ç—…å®³ç±»å‹ç»Ÿè®¡
            if disease_name not in disease_stats:
                disease_stats[disease_name] = 0
            disease_stats[disease_name] += count
            
        # ä¿å­˜ç»Ÿè®¡ç»“æœ
        stats = {
            'total_classes': len(self.class_info),
            'total_images': self.total_images,
            'crop_stats': crop_stats,
            'disease_stats': disease_stats,
            'class_info': self.class_info
        }
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        stats_file = Path("model-training/dataset_statistics.json")
        stats_file.parent.mkdir(exist_ok=True)
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
            
        print(f"ğŸ“„ ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file}")
        return stats
        
    def visualize_statistics(self, stats):
        """å¯è§†åŒ–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        print("\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path("model-training/analysis_plots")
        output_dir.mkdir(exist_ok=True)
        
        # 1. ä½œç‰©ç±»å‹åˆ†å¸ƒ
        plt.figure(figsize=(12, 8))
        crop_names = list(stats['crop_stats'].keys())
        crop_counts = [stats['crop_stats'][crop]['images'] for crop in crop_names]
        
        plt.subplot(2, 2, 1)
        plt.pie(crop_counts, labels=crop_names, autopct='%1.1f%%', startangle=90)
        plt.title('ä½œç‰©ç±»å‹åˆ†å¸ƒ')
        
        # 2. æ¯ä¸ªç±»åˆ«çš„å›¾ç‰‡æ•°é‡
        plt.subplot(2, 2, 2)
        class_names = list(self.class_info.keys())
        class_counts = [self.class_info[cls]['image_count'] for cls in class_names]
        
        plt.bar(range(len(class_names)), class_counts)
        plt.title('å„ç±»åˆ«å›¾ç‰‡æ•°é‡')
        plt.xlabel('ç±»åˆ«')
        plt.ylabel('å›¾ç‰‡æ•°é‡')
        plt.xticks(range(len(class_names)), class_names, rotation=90, fontsize=8)
        
        # 3. å¥åº·vsç—…å®³åˆ†å¸ƒ
        plt.subplot(2, 2, 3)
        healthy_count = sum([info['image_count'] for name, info in self.class_info.items() if 'healthy' in name.lower()])
        diseased_count = self.total_images - healthy_count
        
        plt.pie([healthy_count, diseased_count], labels=['å¥åº·', 'ç—…å®³'], autopct='%1.1f%%', startangle=90)
        plt.title('å¥åº· vs ç—…å®³åˆ†å¸ƒ')
        
        # 4. æ•°æ®å¹³è¡¡æ€§åˆ†æ
        plt.subplot(2, 2, 4)
        plt.hist(class_counts, bins=20, edgecolor='black')
        plt.title('ç±»åˆ«æ•°æ®åˆ†å¸ƒ')
        plt.xlabel('å›¾ç‰‡æ•°é‡')
        plt.ylabel('ç±»åˆ«æ•°')
        
        plt.tight_layout()
        plt.savefig(output_dir / 'dataset_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {output_dir}")
        
    def sample_images_analysis(self):
        """åˆ†ææ ·æœ¬å›¾åƒçš„åŸºæœ¬ä¿¡æ¯"""
        print("\nğŸ–¼ï¸ åˆ†ææ ·æœ¬å›¾åƒ...")
        
        image_stats = {
            'widths': [],
            'heights': [],
            'channels': [],
            'formats': []
        }
        
        # éšæœºé‡‡æ ·ä¸€äº›å›¾åƒè¿›è¡Œåˆ†æ
        sample_count = 0
        max_samples = 100  # é™åˆ¶é‡‡æ ·æ•°é‡ä»¥èŠ‚çœæ—¶é—´
        
        for class_name, info in self.class_info.items():
            if sample_count >= max_samples:
                break
                
            class_path = Path(info['path'])
            image_files = list(class_path.glob("*.jpg")) + list(class_path.glob("*.jpeg")) + list(class_path.glob("*.png"))
            
            # ä»æ¯ä¸ªç±»åˆ«é‡‡æ ·2-3å¼ å›¾ç‰‡
            sample_size = min(3, len(image_files))
            sampled_files = np.random.choice(image_files, sample_size, replace=False)
            
            for img_file in sampled_files:
                if sample_count >= max_samples:
                    break
                    
                try:
                    img = cv2.imread(str(img_file))
                    if img is not None:
                        h, w, c = img.shape
                        image_stats['heights'].append(h)
                        image_stats['widths'].append(w)
                        image_stats['channels'].append(c)
                        image_stats['formats'].append(img_file.suffix.lower())
                        sample_count += 1
                except Exception as e:
                    print(f"âš ï¸ æ— æ³•è¯»å–å›¾åƒ {img_file}: {e}")
                    
        # ç»Ÿè®¡å›¾åƒä¿¡æ¯
        if image_stats['widths']:
            print(f"ğŸ“ é‡‡æ ·å›¾åƒç»Ÿè®¡ (åŸºäº {sample_count} å¼ å›¾ç‰‡):")
            print(f"  å®½åº¦: {np.min(image_stats['widths'])} - {np.max(image_stats['widths'])} (å¹³å‡: {np.mean(image_stats['widths']):.1f})")
            print(f"  é«˜åº¦: {np.min(image_stats['heights'])} - {np.max(image_stats['heights'])} (å¹³å‡: {np.mean(image_stats['heights']):.1f})")
            print(f"  é€šé“æ•°: {Counter(image_stats['channels'])}")
            print(f"  æ ¼å¼: {Counter(image_stats['formats'])}")
            
        return image_stats
        
    def create_yolo_format_structure(self):
        """åˆ›å»ºYOLOæ ¼å¼çš„æ•°æ®é›†ç»“æ„"""
        print("\nğŸ¯ åˆ›å»ºYOLOè®­ç»ƒæ•°æ®ç»“æ„...")
        
        # åˆ›å»ºYOLOæ ¼å¼ç›®å½•
        yolo_dir = Path("model-training/yolo_dataset")
        
        # åˆ›å»ºç›®å½•ç»“æ„
        dirs_to_create = [
            yolo_dir / "train" / "images",
            yolo_dir / "train" / "labels", 
            yolo_dir / "val" / "images",
            yolo_dir / "val" / "labels",
            yolo_dir / "test" / "images",
            yolo_dir / "test" / "labels"
        ]
        
        for dir_path in dirs_to_create:
            dir_path.mkdir(parents=True, exist_ok=True)
            
        # åˆ›å»ºç±»åˆ«æ˜ å°„æ–‡ä»¶
        class_names = sorted(self.class_info.keys())
        class_mapping = {name: idx for idx, name in enumerate(class_names)}
        
        # ä¿å­˜ç±»åˆ«æ˜ å°„
        with open(yolo_dir / "classes.txt", 'w', encoding='utf-8') as f:
            for class_name in class_names:
                f.write(f"{class_name}\n")
                
        # åˆ›å»ºYOLOæ•°æ®é…ç½®æ–‡ä»¶
        yaml_content = f"""# æ¤ç‰©ç—…å®³æ£€æµ‹æ•°æ®é›†é…ç½®
path: {yolo_dir.absolute()}
train: train/images
val: val/images
test: test/images

# ç±»åˆ«æ•°é‡
nc: {len(class_names)}

# ç±»åˆ«åç§°
names: {class_names}
"""
        
        with open(yolo_dir / "dataset.yaml", 'w', encoding='utf-8') as f:
            f.write(yaml_content)
            
        print(f"ğŸ“‚ YOLOæ•°æ®é›†ç»“æ„å·²åˆ›å»º: {yolo_dir}")
        print(f"ğŸ“‹ ç±»åˆ«æ˜ å°„å·²ä¿å­˜: {yolo_dir / 'classes.txt'}")
        print(f"âš™ï¸ YOLOé…ç½®å·²ä¿å­˜: {yolo_dir / 'dataset.yaml'}")
        
        return yolo_dir, class_mapping
        
    def generate_report(self, stats):
        """ç”Ÿæˆæ•°æ®é›†åˆ†ææŠ¥å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        
        report = f"""# æ¤ç‰©ç—…å®³æ•°æ®é›†åˆ†ææŠ¥å‘Š

## ğŸ“Š æ•°æ®é›†æ¦‚è§ˆ
- **æ€»ç±»åˆ«æ•°**: {stats['total_classes']} ä¸ª
- **æ€»å›¾ç‰‡æ•°**: {stats['total_images']:,} å¼ 
- **æ•°æ®é›†è·¯å¾„**: {self.dataset_path}

## ğŸŒ± ä½œç‰©ç±»å‹åˆ†å¸ƒ
"""
        
        for crop, info in sorted(stats['crop_stats'].items()):
            report += f"- **{crop}**: {info['classes']} ä¸ªç±»åˆ«, {info['images']:,} å¼ å›¾ç‰‡\n"
            
        report += f"""
## ğŸ¦  ç—…å®³ç±»å‹ç»Ÿè®¡
å…±å‘ç° {len(stats['disease_stats'])} ç§ä¸åŒçš„ç—…å®³ç±»å‹ï¼š
"""
        
        # æ˜¾ç¤ºå‰10ä¸ªæœ€å¸¸è§çš„ç—…å®³
        top_diseases = sorted(stats['disease_stats'].items(), key=lambda x: x[1], reverse=True)[:10]
        for disease, count in top_diseases:
            report += f"- **{disease}**: {count:,} å¼ å›¾ç‰‡\n"
            
        report += f"""
## ğŸ“ˆ æ•°æ®è´¨é‡è¯„ä¼°
### æ•°æ®å¹³è¡¡æ€§
- æœ€å¤šå›¾ç‰‡çš„ç±»åˆ«: {max(stats['class_info'].values(), key=lambda x: x['image_count'])['image_count']:,} å¼ 
- æœ€å°‘å›¾ç‰‡çš„ç±»åˆ«: {min(stats['class_info'].values(), key=lambda x: x['image_count'])['image_count']:,} å¼ 
- å¹³å‡æ¯ç±»å›¾ç‰‡æ•°: {stats['total_images'] / stats['total_classes']:.0f} å¼ 

### å¥åº·æ ·æœ¬æ¯”ä¾‹
- å¥åº·æ ·æœ¬: {sum([info['image_count'] for name, info in stats['class_info'].items() if 'healthy' in name.lower()]):,} å¼ 
- ç—…å®³æ ·æœ¬: {stats['total_images'] - sum([info['image_count'] for name, info in stats['class_info'].items() if 'healthy' in name.lower()]):,} å¼ 

## ğŸ¯ YOLOè®­ç»ƒå»ºè®®
1. **æ•°æ®é¢„å¤„ç†**: å»ºè®®å°†å›¾åƒresizeåˆ°640x640åƒç´ 
2. **æ•°æ®å¢å¼º**: å¯ä½¿ç”¨æ—‹è½¬ã€ç¿»è½¬ã€äº®åº¦è°ƒæ•´ç­‰æŠ€æœ¯
3. **è®­ç»ƒç­–ç•¥**: å»ºè®®ä½¿ç”¨7:2:1çš„è®­ç»ƒ/éªŒè¯/æµ‹è¯•åˆ’åˆ†
4. **æ¨¡å‹é€‰æ‹©**: æ¨èä½¿ç”¨YOLOv8næˆ–YOLOv8sä½œä¸ºbaseline

## ğŸ“ æ³¨æ„äº‹é¡¹
- æ•°æ®é›†åŒ…å«å¢å¼ºåçš„æ•°æ®ï¼Œè®­ç»ƒæ—¶éœ€æ³¨æ„é¿å…è¿‡æ‹Ÿåˆ
- å»ºè®®å¯¹å›¾åƒè´¨é‡è¿›è¡Œäººå·¥æŠ½æŸ¥
- å¯è€ƒè™‘ä½¿ç”¨è¿ç§»å­¦ä¹ åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = Path("model-training/dataset_analysis_report.md")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
            
        print(f"ğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ± æ¤ç‰©ç—…å®³æ•°æ®é›†åˆ†æå·¥å…·")
    print("=" * 50)
    
    # æ•°æ®é›†è·¯å¾„
    dataset_path = r"F:\photos\Data for Identification of Plant Leaf Diseases Using a 9-layer Deep Convolutional Neural Network\Plant_leaf_diseases_dataset_with_augmentation\Plant_leave_diseases_dataset_with_augmentation"
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = DatasetAnalyzer(dataset_path)
    
    # åˆ†ææ•°æ®é›†ç»“æ„
    if not analyzer.analyze_dataset_structure():
        return
        
    # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    stats = analyzer.generate_statistics()
    
    # å¯è§†åŒ–ç»Ÿè®¡ä¿¡æ¯
    analyzer.visualize_statistics(stats)
    
    # åˆ†ææ ·æœ¬å›¾åƒ
    analyzer.sample_images_analysis()
    
    # åˆ›å»ºYOLOæ ¼å¼ç»“æ„
    analyzer.create_yolo_format_structure()
    
    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    analyzer.generate_report(stats)
    
    print("\nâœ… æ•°æ®é›†åˆ†æå®Œæˆ!")
    print("ğŸ“ æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - model-training/dataset_statistics.json")
    print("  - model-training/analysis_plots/dataset_analysis.png")
    print("  - model-training/dataset_analysis_report.md")
    print("  - model-training/yolo_dataset/")


if __name__ == "__main__":
    main()
